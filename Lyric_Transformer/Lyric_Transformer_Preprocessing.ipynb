{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyric_Transformer_Preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOErZmqX3JUyKOHmLvGlDx5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rnlcksgdkd/test_GAN/blob/master/Lyric_Transformer/Lyric_Transformer_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiZ-fRf8CSkB"
      },
      "source": [
        "# **데이터 로드 및 변환**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdY9hKuS_odK",
        "outputId": "45399dea-40ff-4f14-dd83-86451ac4578b"
      },
      "source": [
        "!git clone https://github.com/Rnlcksgdkd/test_GAN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'test_GAN'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Total 153 (delta 0), reused 0 (delta 0), pack-reused 153\u001b[K\n",
            "Receiving objects: 100% (153/153), 91.74 MiB | 23.20 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "Checking out files: 100% (105/105), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "e6WTa7tF_p1J",
        "outputId": "f48b5b07-3eed-4bcf-9f9b-3f4916812705"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATA_PATH = \"/content/test_GAN/data/\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH + \"Crawling_Lyrics.csv\")\n",
        "df = df.dropna(axis=0)\n",
        "df.info()\n",
        "df.head()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 700 entries, 0 to 800\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   title   700 non-null    object\n",
            " 1   singer  700 non-null    object\n",
            " 2   lyrics  700 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 21.9+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>singer</th>\n",
              "      <th>lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>너희는 (Feat. 김광률)</td>\n",
              "      <td>이은영</td>\n",
              "      <td>너희는 하나님의 택하신\\n거룩하고 사랑스러운 자니 \\n긍율과 자비와 \\n겸손과 온유...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>봄날은 간다 (Bonus Track)</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>눈을 감으면 문득 그리운 날의 기억 \\n아직까지도 마음이 저려 오는 건 \\n\\n그건...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Going Home</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>집으로 돌아가는 길에\\n지는 햇살에 마음을 맡기고\\n나는 너의 일을 떠올리며\\n수많...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>야상곡 (夜想曲)</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>바람이 부는 것은 더운 내 맘 삭여주려 \\n계절이 다 가도록 나는 애만 태우네 \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>길</td>\n",
              "      <td>김윤아</td>\n",
              "      <td>아무도 가르쳐 주지 않아\\n이 길이 옳은지 다른 길로 가야 할지\\n난 저길 저 끝에...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  title  ...                                             lyrics\n",
              "0       너희는 (Feat. 김광률)  ...  너희는 하나님의 택하신\\n거룩하고 사랑스러운 자니 \\n긍율과 자비와 \\n겸손과 온유...\n",
              "1  봄날은 간다 (Bonus Track)  ...  눈을 감으면 문득 그리운 날의 기억 \\n아직까지도 마음이 저려 오는 건 \\n\\n그건...\n",
              "2            Going Home  ...  집으로 돌아가는 길에\\n지는 햇살에 마음을 맡기고\\n나는 너의 일을 떠올리며\\n수많...\n",
              "3             야상곡 (夜想曲)  ...  바람이 부는 것은 더운 내 맘 삭여주려 \\n계절이 다 가도록 나는 애만 태우네 \\n...\n",
              "4                     길  ...  아무도 가르쳐 주지 않아\\n이 길이 옳은지 다른 길로 가야 할지\\n난 저길 저 끝에...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no0e87gN_0yV",
        "outputId": "b167a442-bd82-4678-ef2c-4cd9c10a1af5"
      },
      "source": [
        "lyric_data = df['lyrics'].values.tolist()\n",
        "len(lyric_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gUCXhrHAB3n"
      },
      "source": [
        "Lyrics = []\n",
        "for lyric in LYRICS :\n",
        "    lst = [token for token in lyric.split(\"\\n\") if token != \"\" and token != \" \"]\n",
        "    Lyrics.append(lst)\n",
        "\n",
        "Lyrics[400]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8OTRVH2A9pp"
      },
      "source": [
        "Lyric_input = []\n",
        "Lyric_output = []\n",
        "\n",
        "for lyric in Lyrics:\n",
        "  for i, words in enumerate(lyric):\n",
        "    if i + 1 < len(lyric):\n",
        "      Lyric_input.append(words)\n",
        "      Lyric_output.append(lyric[i + 1])\n",
        "    else:\n",
        "      break\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeqW-cOgBUlg",
        "outputId": "4b0bbd1b-9e1e-4932-86f2-885cfc48d7b8"
      },
      "source": [
        "for i in range(120 , 130):\n",
        "  print(Lyric_input[i] + \"///\" + Lyric_output[i])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "내가 많이 미안해///너에게\n",
            "너에게///그래서 더 아파\n",
            "그래서 더 아파///내가 미워져\n",
            "내가 미워져///미안해\n",
            "미안해///이 말 밖에 없는\n",
            "이 말 밖에 없는///한 번만 더 내 이름\n",
            "한 번만 더 내 이름///불러 줄래\n",
            "불러 줄래///한 번만 더 곁에\n",
            "한 번만 더 곁에///있어 줄래\n",
            "있어 줄래///사랑해 사랑한\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mczeycOCCXua"
      },
      "source": [
        "# **데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1iTKgAHCd3K",
        "outputId": "0e1abda6-8887-4eff-bc94-f6185f64dce2"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 45.5MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFod5n73CgKb"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PAD>\"\n",
        "STD = \"<SOS>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "MAX_SEQUENCE = 25\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "  # 판다스를 통해서 데이터를 불러온다.\n",
        "  data_df = pd.read_csv(path, header=0)\n",
        "  # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
        "  question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "  return question, answer\n",
        "\n",
        "\n",
        "def data_tokenizer(data):\n",
        "  # 토크나이징 해서 담을 배열 생성\n",
        "  words = []\n",
        "  for sentence in data:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 위 필터와 같은 값들을 정규화 표현식을\n",
        "      # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
        "      sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "      for word in sentence.split():\n",
        "          words.append(word)\n",
        "  # 토그나이징과 정규표현식을 통해 만들어진\n",
        "  # 값들을 넘겨 준다.\n",
        "  return [word for word in words if word]\n",
        "\n",
        "\n",
        "def prepro_like_morphlized(data):\n",
        "  morph_analyzer = Okt()\n",
        "  result_data = list()\n",
        "  for seq in tqdm(data):\n",
        "      morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "      result_data.append(morphlized_seq)\n",
        "\n",
        "  return result_data\n",
        "\n",
        "\n",
        "def load_vocabulary(input , output ,  vocab_path, tokenize_as_morph=False):\n",
        "\n",
        "    \n",
        "  question, answer = input , output\n",
        "\n",
        "  if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
        "      question = prepro_like_morphlized(question)\n",
        "      answer = prepro_like_morphlized(answer)\n",
        "  data = []\n",
        "\n",
        "  data.extend(question)\n",
        "  data.extend(answer)\n",
        "\n",
        "  words = data_tokenizer(data)\n",
        "\n",
        "  words = list(set(words))\n",
        "\n",
        "  words[:0] = MARKER\n",
        "\n",
        "  with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
        "      for word in words:\n",
        "          vocabulary_file.write(word + '\\n')\n",
        "\n",
        "  vocabulary_list = []\n",
        "  with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "      for line in vocabulary_file:\n",
        "          vocabulary_list.append(line.strip())\n",
        "\n",
        "  # 배열에 내용을 키와 값이 있는\n",
        "  # 딕셔너리 구조로 만든다.\n",
        "  char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
        "  # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
        "  # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "  return char2idx, idx2char, len(char2idx)     \n",
        "\n",
        "\n",
        "def make_vocabulary(vocabulary_list):\n",
        "  # 리스트를 키가 단어이고 값이 인덱스인\n",
        "  # 딕셔너리를 만든다.\n",
        "  char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
        "  # 리스트를 키가 인덱스이고 값이 단어인\n",
        "  # 딕셔너리를 만든다.\n",
        "  idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
        "  # 두개의 딕셔너리를 넘겨 준다.\n",
        "  return char2idx, idx2char\n",
        "\n",
        "\n",
        "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
        "  # 인덱스 값들을 가지고 있는\n",
        "  # 배열이다.(누적된다.)\n",
        "  sequences_input_index = []\n",
        "  # 하나의 인코딩 되는 문장의\n",
        "  # 길이를 가지고 있다.(누적된다.)\n",
        "  sequences_length = []\n",
        "  # 형태소 토크나이징 사용 유무\n",
        "  if tokenize_as_morph:\n",
        "      value = prepro_like_morphlized(value)\n",
        "\n",
        "  # 한줄씩 불어온다.\n",
        "  for sequence in value:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 정규화를 사용하여 필터에 들어 있는\n",
        "      # 값들을 \"\" 으로 치환 한다.\n",
        "      sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "      # 하나의 문장을 인코딩 할때\n",
        "      # 가지고 있기 위한 배열이다.\n",
        "      sequence_index = []\n",
        "      # 문장을 스페이스 단위로\n",
        "      # 자르고 있다.\n",
        "      for word in sequence.split():\n",
        "          # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
        "          # 그 값을 가져와 sequence_index에 추가한다.\n",
        "          if dictionary.get(word) is not None:\n",
        "              sequence_index.extend([dictionary[word]])\n",
        "          # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
        "          # 경우 이므로 UNK(2)를 넣어 준다.\n",
        "          else:\n",
        "              sequence_index.extend([dictionary[UNK]])\n",
        "      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "      if len(sequence_index) > MAX_SEQUENCE:\n",
        "          sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "      # 하나의 문장에 길이를 넣어주고 있다.\n",
        "      sequences_length.append(len(sequence_index))\n",
        "      # max_sequence_length보다 문장 길이가\n",
        "      # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "      # 인덱스화 되어 있는 값을\n",
        "      # sequences_input_index에 넣어 준다.\n",
        "      sequences_input_index.append(sequence_index)\n",
        "  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "  # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "  # 사전 작업이다.\n",
        "  # 넘파이 배열에 인덱스화된 배열과\n",
        "  # 그 길이를 넘겨준다.\n",
        "  return np.asarray(sequences_input_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
        "  # 인덱스 값들을 가지고 있는\n",
        "  # 배열이다.(누적된다)\n",
        "  sequences_output_index = []\n",
        "  # 하나의 디코딩 입력 되는 문장의\n",
        "  # 길이를 가지고 있다.(누적된다)\n",
        "  sequences_length = []\n",
        "  # 형태소 토크나이징 사용 유무\n",
        "  if tokenize_as_morph:\n",
        "      value = prepro_like_morphlized(value)\n",
        "  # 한줄씩 불어온다.\n",
        "  for sequence in value:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 정규화를 사용하여 필터에 들어 있는\n",
        "      # 값들을 \"\" 으로 치환 한다.\n",
        "      sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "      # 하나의 문장을 디코딩 할때 가지고\n",
        "      # 있기 위한 배열이다.\n",
        "      sequence_index = []\n",
        "      # 디코딩 입력의 처음에는 START가 와야 하므로\n",
        "      # 그 값을 넣어 주고 시작한다.\n",
        "      # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
        "      # 값인 인덱스를 넣어 준다.\n",
        "      sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "      if len(sequence_index) > MAX_SEQUENCE:\n",
        "          sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "      # 하나의 문장에 길이를 넣어주고 있다.\n",
        "      sequences_length.append(len(sequence_index))\n",
        "      # max_sequence_length보다 문장 길이가\n",
        "      # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "      # 인덱스화 되어 있는 값을\n",
        "      # sequences_output_index 넣어 준다.\n",
        "      sequences_output_index.append(sequence_index)\n",
        "  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "  # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "  # 사전 작업이다.\n",
        "  # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "  return np.asarray(sequences_output_index), sequences_length\n",
        "\n",
        "\n",
        "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
        "  # 인덱스 값들을 가지고 있는\n",
        "  # 배열이다.(누적된다)\n",
        "  sequences_target_index = []\n",
        "  # 형태소 토크나이징 사용 유무\n",
        "  if tokenize_as_morph:\n",
        "      value = prepro_like_morphlized(value)\n",
        "  # 한줄씩 불어온다.\n",
        "  for sequence in value:\n",
        "      # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "      # 정규화를 사용하여 필터에 들어 있는\n",
        "      # 값들을 \"\" 으로 치환 한다.\n",
        "      sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "      # 문장에서 스페이스 단위별로 단어를 가져와서\n",
        "      # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
        "      # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
        "      sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "      # 그리고 END 토큰을 넣어 준다\n",
        "      if len(sequence_index) >= MAX_SEQUENCE:\n",
        "          sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
        "      else:\n",
        "          sequence_index += [dictionary[END]]\n",
        "      # max_sequence_length보다 문장 길이가\n",
        "      # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "      # 인덱스화 되어 있는 값을\n",
        "      # sequences_target_index에 넣어 준다.\n",
        "      sequences_target_index.append(sequence_index)\n",
        "  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "  # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
        "  # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "  return np.asarray(sequences_target_index)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwZRVlYtDWX0"
      },
      "source": [
        "VOCAB_PATH = '/content/vocabulary.txt'\n",
        "\n",
        "char2idx, idx2char, vocab_size = load_vocabulary(Lyric_input , Lyric_output , VOCAB_PATH, tokenize_as_morph=False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEHuM5yqCojd"
      },
      "source": [
        "index_inputs, input_seq_len = enc_processing(Lyric_input, char2idx, tokenize_as_morph=False)\n",
        "index_outputs, output_seq_len = dec_output_processing(Lyric_output, char2idx, tokenize_as_morph=False)\n",
        "index_targets = dec_target_processing(Lyric_output, char2idx, tokenize_as_morph=False)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGqKKcdtDDQr",
        "outputId": "d0c6ea51-fd36-4857-8bc6-03bc64a63efd"
      },
      "source": [
        "index_inputs.shape , index_outputs.shape , index_targets.shape , vocab_size , len(char2idx)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17589, 25), (17589, 25), (17589, 25), 15843, 15843)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5nMnvq0DGMU"
      },
      "source": [
        "\n",
        "data_configs = {}\n",
        "data_configs['char2idx'] = char2idx\n",
        "data_configs['idx2char'] = idx2char\n",
        "data_configs['vocab_size'] = vocab_size\n",
        "data_configs['pad_symbol'] = PAD\n",
        "data_configs['std_symbol'] = STD\n",
        "data_configs['end_symbol'] = END\n",
        "data_configs['unk_symbol'] = UNK\n",
        "\n",
        "DATA_IN_PATH = '/content/'\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'wb'), index_outputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_TARGETS , 'wb'), index_targets)\n",
        "\n",
        "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drAB_OBNDIJo",
        "outputId": "9ccd0fb9-4bfc-41ea-dd34-ae3ee9e08625"
      },
      "source": [
        "%cd /content/test_GAN\n",
        "!git config --global user.email \"dksehgis@naver.com\"\n",
        "!git config --global user.name \"ando\"\n",
        "!git add .\n",
        "!git commit -m \"update Transformer_Preprocessing\"\n",
        "!git push  https://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@github.com/Rnlcksgdkd/test_GAN.git"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/test_GAN\n",
            "[master 0440648] update Transformer_Preprocessing\n",
            " 5 files changed, 15844 insertions(+)\n",
            " create mode 100644 Lyric_Transformer/data_configs.json\n",
            " create mode 100644 Lyric_Transformer/train_inputs.npy\n",
            " create mode 100644 Lyric_Transformer/train_outputs.npy\n",
            " create mode 100644 Lyric_Transformer/train_targets.npy\n",
            " create mode 100644 Lyric_Transformer/vocabulary.txt\n",
            "Counting objects: 8, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (8/8), done.\n",
            "Writing objects: 100% (8/8), 630.06 KiB | 1.64 MiB/s, done.\n",
            "Total 8 (delta 3), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (3/3), completed with 1 local object.\u001b[K\n",
            "To https://github.com/Rnlcksgdkd/test_GAN.git\n",
            "   2dbcc6c..0440648  master -> master\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}