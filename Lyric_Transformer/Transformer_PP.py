# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xPgbKWWSuNYmQGI4Xe-XNDmEfzs8p5It
"""

import os
import re
import json

import numpy as np
import pandas as pd
from tqdm import tqdm

from konlpy.tag import Okt


FILTERS = "([~.,!?\"':;)(])"
PAD = "<PAD>"
STD = "<SOS>"
END = "<END>"
UNK = "<UNK>"

PAD_INDEX = 0
STD_INDEX = 1
END_INDEX = 2
UNK_INDEX = 3

MARKER = [PAD, STD, END, UNK]
CHANGE_FILTER = re.compile(FILTERS)

MAX_SEQUENCE = 25


def load_data(path):
  # 판다스를 통해서 데이터를 불러온다.
  data_df = pd.read_csv(path, header=0)
  # 질문과 답변 열을 가져와 question과 answer에 넣는다.
  question, answer = list(data_df['Q']), list(data_df['A'])

  return question, answer


def data_tokenizer(data):
  # 토크나이징 해서 담을 배열 생성
  words = []
  for sentence in data:
      # FILTERS = "([~.,!?\"':;)(])"
      # 위 필터와 같은 값들을 정규화 표현식을
      # 통해서 모두 "" 으로 변환 해주는 부분이다.
      sentence = re.sub(CHANGE_FILTER, "", sentence)
      for word in sentence.split():
          words.append(word)
  # 토그나이징과 정규표현식을 통해 만들어진
  # 값들을 넘겨 준다.
  return [word for word in words if word]


def prepro_like_morphlized(data):
  morph_analyzer = Okt()
  result_data = list()
  for seq in tqdm(data):
      morphlized_seq = " ".join(morph_analyzer.morphs(seq.replace(' ', '')))
      result_data.append(morphlized_seq)

  return result_data


def load_vocabulary(input , output ,  vocab_path, tokenize_as_morph=False):

    
  question, answer = input , output

  if tokenize_as_morph:  # 형태소에 따른 토크나이져 처리
      question = prepro_like_morphlized(question)
      answer = prepro_like_morphlized(answer)
  data = []

  data.extend(question)
  data.extend(answer)

  words = data_tokenizer(data)

  words = list(set(words))

  words[:0] = MARKER

  with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:
      for word in words:
          vocabulary_file.write(word + '\n')

  vocabulary_list = []
  with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:
      for line in vocabulary_file:
          vocabulary_list.append(line.strip())

  # 배열에 내용을 키와 값이 있는
  # 딕셔너리 구조로 만든다.
  char2idx, idx2char = make_vocabulary(vocabulary_list)
  # 두가지 형태의 키와 값이 있는 형태를 리턴한다.
  # (예) 단어: 인덱스 , 인덱스: 단어)
  return char2idx, idx2char, len(char2idx)     


def make_vocabulary(vocabulary_list):
  # 리스트를 키가 단어이고 값이 인덱스인
  # 딕셔너리를 만든다.
  char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}
  # 리스트를 키가 인덱스이고 값이 단어인
  # 딕셔너리를 만든다.
  idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}
  # 두개의 딕셔너리를 넘겨 준다.
  return char2idx, idx2char


def enc_processing(value, dictionary, tokenize_as_morph=False):
  # 인덱스 값들을 가지고 있는
  # 배열이다.(누적된다.)
  sequences_input_index = []
  # 하나의 인코딩 되는 문장의
  # 길이를 가지고 있다.(누적된다.)
  sequences_length = []
  # 형태소 토크나이징 사용 유무
  if tokenize_as_morph:
      value = prepro_like_morphlized(value)

  # 한줄씩 불어온다.
  for sequence in value:
      # FILTERS = "([~.,!?\"':;)(])"
      # 정규화를 사용하여 필터에 들어 있는
      # 값들을 "" 으로 치환 한다.
      sequence = re.sub(CHANGE_FILTER, "", sequence)
      # 하나의 문장을 인코딩 할때
      # 가지고 있기 위한 배열이다.
      sequence_index = []
      # 문장을 스페이스 단위로
      # 자르고 있다.
      for word in sequence.split():
          # 잘려진 단어들이 딕셔너리에 존재 하는지 보고
          # 그 값을 가져와 sequence_index에 추가한다.
          if dictionary.get(word) is not None:
              sequence_index.extend([dictionary[word]])
          # 잘려진 단어가 딕셔너리에 존재 하지 않는
          # 경우 이므로 UNK(2)를 넣어 준다.
          else:
              sequence_index.extend([dictionary[UNK]])
      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.
      if len(sequence_index) > MAX_SEQUENCE:
          sequence_index = sequence_index[:MAX_SEQUENCE]
      # 하나의 문장에 길이를 넣어주고 있다.
      sequences_length.append(len(sequence_index))
      # max_sequence_length보다 문장 길이가
      # 작다면 빈 부분에 PAD(0)를 넣어준다.
      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]
      # 인덱스화 되어 있는 값을
      # sequences_input_index에 넣어 준다.
      sequences_input_index.append(sequence_index)
  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.
  # 이유는 텐서플로우 dataset에 넣어 주기 위한
  # 사전 작업이다.
  # 넘파이 배열에 인덱스화된 배열과
  # 그 길이를 넘겨준다.
  return np.asarray(sequences_input_index), sequences_length


def dec_output_processing(value, dictionary, tokenize_as_morph=False):
  # 인덱스 값들을 가지고 있는
  # 배열이다.(누적된다)
  sequences_output_index = []
  # 하나의 디코딩 입력 되는 문장의
  # 길이를 가지고 있다.(누적된다)
  sequences_length = []
  # 형태소 토크나이징 사용 유무
  if tokenize_as_morph:
      value = prepro_like_morphlized(value)
  # 한줄씩 불어온다.
  for sequence in value:
      # FILTERS = "([~.,!?\"':;)(])"
      # 정규화를 사용하여 필터에 들어 있는
      # 값들을 "" 으로 치환 한다.
      sequence = re.sub(CHANGE_FILTER, "", sequence)
      # 하나의 문장을 디코딩 할때 가지고
      # 있기 위한 배열이다.
      sequence_index = []
      # 디코딩 입력의 처음에는 START가 와야 하므로
      # 그 값을 넣어 주고 시작한다.
      # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의
      # 값인 인덱스를 넣어 준다.
      sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]
      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.
      if len(sequence_index) > MAX_SEQUENCE:
          sequence_index = sequence_index[:MAX_SEQUENCE]
      # 하나의 문장에 길이를 넣어주고 있다.
      sequences_length.append(len(sequence_index))
      # max_sequence_length보다 문장 길이가
      # 작다면 빈 부분에 PAD(0)를 넣어준다.
      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]
      # 인덱스화 되어 있는 값을
      # sequences_output_index 넣어 준다.
      sequences_output_index.append(sequence_index)
  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.
  # 이유는 텐서플로우 dataset에 넣어 주기 위한
  # 사전 작업이다.
  # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.
  return np.asarray(sequences_output_index), sequences_length


def dec_target_processing(value, dictionary, tokenize_as_morph=False):
  # 인덱스 값들을 가지고 있는
  # 배열이다.(누적된다)
  sequences_target_index = []
  # 형태소 토크나이징 사용 유무
  if tokenize_as_morph:
      value = prepro_like_morphlized(value)
  # 한줄씩 불어온다.
  for sequence in value:
      # FILTERS = "([~.,!?\"':;)(])"
      # 정규화를 사용하여 필터에 들어 있는
      # 값들을 "" 으로 치환 한다.
      sequence = re.sub(CHANGE_FILTER, "", sequence)
      # 문장에서 스페이스 단위별로 단어를 가져와서
      # 딕셔너리의 값인 인덱스를 넣어 준다.
      # 디코딩 출력의 마지막에 END를 넣어 준다.
      sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]
      # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.
      # 그리고 END 토큰을 넣어 준다
      if len(sequence_index) >= MAX_SEQUENCE:
          sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]
      else:
          sequence_index += [dictionary[END]]
      # max_sequence_length보다 문장 길이가
      # 작다면 빈 부분에 PAD(0)를 넣어준다.
      sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]
      # 인덱스화 되어 있는 값을
      # sequences_target_index에 넣어 준다.
      sequences_target_index.append(sequence_index)
  # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.
  # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.
  # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.
  return np.asarray(sequences_target_index)